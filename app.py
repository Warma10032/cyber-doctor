import base64
from qa.answer import get_answer
from qa.question_parser import parse_question
from qa.function_tool import process_image_describe_tool
from qa.purpose_type import userPurposeType
from audio.audio_generate import audio_generate

import PyPDF2
import chardet
import mimetypes
import gradio as gr
from icecream import ic
from docx import Document
from pydub import AudioSegment
import speech_recognition as sr
from opencc import OpenCC
import os


AVATAR = ("resource/user.png", "resource/bot.jpg")
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# pip install whisper
# pip install openai-whisper
# pip install soundfile
# pip install pydub
# pip install opencc-python-reimplemented


def convert_to_simplified(text):
    converter = OpenCC("t2s")
    return converter.convert(text)


def convert_audio_to_wav(audio_file_path):
    audio = AudioSegment.from_file(audio_file_path)  # è‡ªåŠ¨è¯†åˆ«æ ¼å¼
    wav_file_path = audio_file_path.rsplit(".", 1)[0] + ".wav"  # ç”Ÿæˆ WAV æ–‡ä»¶è·¯å¾„
    audio.export(wav_file_path, format="wav")  # å°†éŸ³é¢‘æ–‡ä»¶å¯¼å‡ºä¸º WAV æ ¼å¼
    return wav_file_path


def audio_to_text(audio_file_path):
    # åˆ›å»ºè¯†åˆ«å™¨å¯¹è±¡
    # å¦‚æœä¸æ˜¯ WAV æ ¼å¼ï¼Œå…ˆè½¬æ¢ä¸º WAV
    if not audio_file_path.endswith(".wav"):
        audio_file_path = convert_audio_to_wav(audio_file_path)

    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_file_path) as source:
        audio_data = recognizer.record(source)
        # ä½¿ç”¨ Google Web Speech API è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œä¸ç”¨ä¸‹è½½æ¨¡å‹ä½†å¯¹ç½‘ç»œè¦æ±‚é«˜
        # text = recognizer.recognize_google(audio_data, language="zh-CN")
        # ä½¿ç”¨ whisper è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œè‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
        text = recognizer.recognize_whisper(audio_data, language="zh")
        text_simplified = convert_to_simplified(text)
    return text_simplified


# pip install PyPDF2
def pdf_to_str(pdf_file):
    reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text


def docx_to_str(file_path):
    doc = Document(file_path)
    text = []
    for paragraph in doc.paragraphs:
        text.append(paragraph.text)
    return "\n".join(text)


# pip install chardet
def text_file_to_str(text_file):
    with open(text_file, "rb") as file:
        raw_data = file.read()
        result = chardet.detect(raw_data)
        encoding = result["encoding"]

    # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç æ¥è¯»å–æ–‡ä»¶
    with open(text_file, "r", encoding=encoding) as file:
        return file.read()


def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode("utf-8")
        return encoded_string


# æ ¸å¿ƒå‡½æ•°
def grodio_view(chatbot, chat_input):

    # ç”¨æˆ·æ¶ˆæ¯ç«‹å³æ˜¾ç¤º
    user_message = chat_input["text"]
    bot_response = "loading..."
    chatbot.append([user_message, bot_response])
    yield chatbot

    # å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶
    files = chat_input["files"]
    audios = []
    images = []
    pdfs = []
    docxs = []
    texts = []

    for file in files:
        file_type, _ = mimetypes.guess_type(file)
        if file_type.startswith("audio/"):
            audios.append(file)
        elif file_type.startswith("image/"):
            images.append(file)
        elif file_type.startswith("application/pdf"):
            pdfs.append(file)
        elif file_type.startswith(
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        ):
            docxs.append(file)
        elif file_type.startswith("text/"):
            texts.append(file)
        else:
            user_message += "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'è¯¥æ–‡ä»¶ä¸ºä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'"
            print(f"Unknown file type: {file_type}")

    # å›¾ç‰‡æ–‡ä»¶è§£æ
    if images != []:
        image_url = images
        image_base64 = [image_to_base64(image) for image in image_url]

        for i, image in enumerate(image_base64):
            chatbot[-1][
                0
            ] += f"""
                <div>
                    <img src="data:image/png;base64,{image}" alt="Generated Image" style="max-width: 100%; height: auto; cursor: pointer;" />
                </div>
                """
            yield chatbot
    else:
        image_url = None

    question_type = parse_question(user_message, image_url)
    ic(question_type)

    # éŸ³é¢‘æ–‡ä»¶è§£æ
    if audios != []:
        for i, audio in enumerate(audios):
            audio_message = audio_to_text(audio)
            if audio_message == "":
                user_message += "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'éŸ³é¢‘è¯†åˆ«å¤±è´¥ï¼Œè¯·ç¨åå†è¯•'"
            elif "ä½œæ›²" in audio_message:
                user_message += "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'ä¸å¥½æ„æ€ï¼Œæˆ‘æ— æ³•ç†è§£éŸ³ä¹'"
            else:
                user_message += f"éŸ³é¢‘{i+1}å†…å®¹ï¼š{audio_message}"

    if pdfs != []:
        for i, pdf in enumerate(pdfs):
            pdf_text = pdf_to_str(pdf)
            user_message += f"PDF{i+1}å†…å®¹ï¼š{pdf_text}"

    if docxs != []:
        for i, docx in enumerate(docxs):
            docx_text = docx_to_str(docx)
            user_message += f"DOCX{i+1}å†…å®¹ï¼š{docx_text}"

    if texts != []:
        for i, text in enumerate(texts):
            text_string = text_file_to_str(text)
            user_message += f"æ–‡æœ¬{i+1}å†…å®¹ï¼š{text_string}"

    if user_message == "":
        user_message = "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'è¯·é—®æ‚¨æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„ï¼Œæˆ‘å°†å°½åŠ›ä¸ºæ‚¨æœåŠ¡'"
    answer = get_answer(user_message, chatbot, question_type, image_url)
    bot_response = ""

    # å¤„ç†æ–‡æœ¬ç”Ÿæˆ/å…¶ä»–/æ–‡æ¡£æ£€ç´¢/çŸ¥è¯†å›¾è°±æ£€ç´¢
    if (
        answer[1] == userPurposeType.text
        or answer[1] == userPurposeType.RAG
        or answer[1] == userPurposeType.KnowledgeGraph
    ):
        # æµå¼è¾“å‡º
        for chunk in answer[0]:
            bot_response = bot_response + (chunk.choices[0].delta.content or "")
            chatbot[-1][1] = bot_response
            yield chatbot

    # å¤„ç†å›¾ç‰‡ç”Ÿæˆ
    if answer[1] == userPurposeType.ImageGeneration:
        image_url = answer[0]
        describe = process_image_describe_tool(
            question_type=userPurposeType.ImageDescribe,
            question="æè¿°è¿™ä¸ªå›¾ç‰‡ï¼Œä¸è¦è¯†åˆ«â€˜AIç”Ÿæˆâ€™",
            history="",
            image_url=[image_url],
        )
        combined_message = f"""
            **ç”Ÿæˆçš„å›¾ç‰‡:**
            ![Generated Image]({image_url})
            {describe[0]}
            """
        chatbot[-1][1] = combined_message
        yield chatbot

    # å¤„ç†å›¾ç‰‡æè¿°
    if answer[1] == userPurposeType.ImageDescribe:
        for i in range(0, len(answer[0]), 1):
            bot_response += answer[0][i : i + 1]  # ç´¯åŠ å½“å‰chunkåˆ°combined_message
            chatbot[-1][1] = bot_response  # æ›´æ–°chatbotå¯¹è¯ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯
            yield chatbot  # å®æ—¶è¾“å‡ºå½“å‰ç´¯ç§¯çš„å¯¹è¯å†…å®¹

    # å¤„ç†è§†é¢‘
    if answer[1] == userPurposeType.Video:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            chatbot[-1][1] = "æŠ±æ­‰ï¼Œè§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†PPT
    if answer[1] == userPurposeType.PPT:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            chatbot[-1][1] = "æŠ±æ­‰ï¼ŒPPTç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†Docx
    if answer[1] == userPurposeType.Docx:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            chatbot[-1][1] = "æŠ±æ­‰ï¼Œæ–‡æ¡£ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†éŸ³é¢‘ç”Ÿæˆ
    if answer[1] == userPurposeType.Audio:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            chatbot[-1][1] = "æŠ±æ­‰ï¼ŒéŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†è”ç½‘æœç´¢
    if answer[1] == userPurposeType.InternetSearch:
        if answer[3] == False:
            output_message = (
                "ç”±äºç½‘ç»œé—®é¢˜ï¼Œè®¿é—®äº’è”ç½‘å¤±è´¥ï¼Œä¸‹é¢ç”±æˆ‘æ ¹æ®ç°æœ‰çŸ¥è¯†ç»™å‡ºå›ç­”ï¼š"
            )
        else:
            # å°†å­—å…¸ä¸­çš„å†…å®¹è½¬æ¢ä¸º Markdown æ ¼å¼çš„é“¾æ¥
            links = "\n".join(f"[{title}]({link})" for link, title in answer[2].items())
            links += "\n"
            output_message = f"å‚è€ƒèµ„æ–™ï¼š{links}"
        for i in range(0, len(output_message)):
            bot_response = output_message[: i + 1]
            chatbot[-1][1] = bot_response
            yield chatbot
        for chunk in answer[0]:
            bot_response = bot_response + (chunk.choices[0].delta.content or "")
            chatbot[-1][1] = bot_response
            yield chatbot


def gradio_audio_view(chatbot, audio_input):

    # ç”¨æˆ·æ¶ˆæ¯ç«‹å³æ˜¾ç¤º
    if audio_input is None:
        user_message = ""
    else:
        user_message = (audio_input, "audio")
    bot_response = "loading..."
    chatbot.append([user_message, bot_response])
    yield chatbot

    if audio_input is None:
        audio_message = "æ— éŸ³é¢‘"
    else:
        audio_message = audio_to_text(audio_input)

    chatbot[-1][0] = audio_message

    user_message = ""
    if audio_message == "æ— éŸ³é¢‘":
        user_message += "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'æ¬¢è¿ä¸æˆ‘å¯¹è¯ï¼Œæˆ‘å°†ç”¨è¯­éŸ³å›ç­”æ‚¨'"
    elif audio_message == "":
        user_message += "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'éŸ³é¢‘è¯†åˆ«å¤±è´¥ï¼Œè¯·ç¨åå†è¯•'"
    elif "ä½œæ›² ä½œæ›²" in audio_message:
        user_message += "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'ä¸å¥½æ„æ€ï¼Œæˆ‘æ— æ³•ç†è§£éŸ³ä¹'"
    else:
        user_message += audio_message

    if user_message == "":
        user_message = "è¯·ä½ å°†ä¸‹é¢çš„å¥å­ä¿®é¥°åè¾“å‡ºï¼Œä¸è¦åŒ…å«é¢å¤–çš„æ–‡å­—ï¼Œå¥å­:'è¯·é—®æ‚¨æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„ï¼Œæˆ‘å°†å°½åŠ›ä¸ºæ‚¨æœåŠ¡'"

    question_type = parse_question(user_message)
    ic(question_type)
    answer = get_answer(user_message, chatbot, question_type)
    bot_response = ""

    # å¤„ç†æ–‡æœ¬ç”Ÿæˆ/å…¶ä»–/æ–‡æ¡£æ£€ç´¢/çŸ¥è¯†å›¾è°±æ£€ç´¢
    if (
        answer[1] == userPurposeType.text
        or answer[1] == userPurposeType.RAG
        or answer[1] == userPurposeType.KnowledgeGraph
    ):
        # è¯­éŸ³è¾“å‡º
        for chunk in answer[0]:
            # è·å–æ¯ä¸ªå—çš„æ•°æ®
            chunk_content = chunk.choices[0].delta.content or ""
            bot_response += chunk_content

        try:
            chatbot[-1][1] = (
                audio_generate(
                    text=bot_response,
                    model_name="zh-CN-YunxiNeural",
                ),
                "audio",
            )
        except Exception as e:
            print(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œç›´æ¥è¿”å›æ–‡æœ¬: {str(e)}")
            chatbot[-1][1] = bot_response 
            
        yield chatbot

    # å¤„ç†å›¾ç‰‡ç”Ÿæˆ
    if answer[1] == userPurposeType.ImageGeneration:
        image_url = answer[0]
        describe = process_image_describe_tool(
            question_type=userPurposeType.ImageDescribe,
            question="æè¿°è¿™ä¸ªå›¾ç‰‡ï¼Œä¸è¦è¯†åˆ«â€˜AIç”Ÿæˆâ€™",
            history=" ",
            image_url=[image_url],
        )
        combined_message = f"""
            **ç”Ÿæˆçš„å›¾ç‰‡:**
            ![Generated Image]({image_url})
            {describe[0]}
            """
        chatbot[-1][1] = combined_message
        yield chatbot

    # å¤„ç†è§†é¢‘
    if answer[1] == userPurposeType.Video:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            try:
                chatbot[-1][1] = (
                    audio_generate(
                        text="æŠ±æ­‰ï¼Œè§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•",
                        model_name="zh-CN-YunxiNeural",
                    ),
                    "audio",
                )
            except Exception as e:
                chatbot[-1][1] = "æŠ±æ­‰ï¼Œè§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†PPT
    if answer[1] == userPurposeType.PPT:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            try:
                chatbot[-1][1] = (
                    audio_generate(
                        text="æŠ±æ­‰ï¼ŒPPTç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•",
                        model_name="zh-CN-YunxiNeural",
                    ),
                    "audio",
                )
            except Exception as e:
                chatbot[-1][1] = "æŠ±æ­‰ï¼ŒPPTç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†Docx
    if answer[1] == userPurposeType.Docx:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            try:
                chatbot[-1][1] = (
                    audio_generate(
                        text="æŠ±æ­‰ï¼Œæ–‡æ¡£ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•",
                        model_name="zh-CN-YunxiNeural",
                    ),
                    "audio",
                )
            except Exception as e:
                chatbot[-1][1] = "æŠ±æ­‰ï¼Œæ–‡æ¡£ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†éŸ³é¢‘ç”Ÿæˆ
    if answer[1] == userPurposeType.Audio:
        if answer[0] is not None:
            chatbot[-1][1] = answer[0]
        else:
            try:
                chatbot[-1][1] = (
                    audio_generate(
                        text="æŠ±æ­‰ï¼ŒéŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•",
                        model_name="zh-CN-YunxiNeural",
                    ),
                    "audio",
                )
            except Exception as e:
                chatbot[-1][1] = "æŠ±æ­‰ï¼ŒéŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        yield chatbot

    # å¤„ç†è”ç½‘æœç´¢
    if answer[1] == userPurposeType.InternetSearch:
        if answer[3] == False:
            bot_response = (
                "ç”±äºç½‘ç»œé—®é¢˜ï¼Œè®¿é—®äº’è”ç½‘å¤±è´¥ï¼Œä¸‹é¢ç”±æˆ‘æ ¹æ®ç°æœ‰çŸ¥è¯†ç»™å‡ºå›ç­”ï¼š"
            )
        # è¯­éŸ³è¾“å‡º
        for chunk in answer[0]:
            # è·å–æ¯ä¸ªå—çš„æ•°æ®
            chunk_content = chunk.choices[0].delta.content or ""
            bot_response += chunk_content

        try:
            chatbot[-1][1] = (
                audio_generate(
                    text=bot_response,
                    model_name="zh-CN-YunxiNeural",
                ),
                "audio",
            )
        except Exception as e:
            print(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œç›´æ¥è¿”å›æ–‡æœ¬: {str(e)}")
            chatbot[-1][1] = bot_response
        yield chatbot


# åˆ‡æ¢åˆ°è¯­éŸ³æ¨¡å¼çš„å‡½æ•°
def toggle_voice_mode():
    return (
        gr.update(visible=False),
        gr.update(visible=True),
        gr.update(visible=False),
        gr.update(visible=True),
        gr.update(visible=True),
    )


# åˆ‡æ¢å›æ–‡æœ¬æ¨¡å¼çš„å‡½æ•°
def toggle_text_mode():
    return (
        gr.update(visible=True),
        gr.update(visible=False),
        gr.update(visible=True),
        gr.update(visible=False),
        gr.update(visible=False),
    )


examples = [
    {"text": "æ‚¨å¥½", "files": []},
    {"text": "ç³–å°¿ç—…çš„å¸¸è§ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ", "files": []},
    {"text": "ç”¨è¯­éŸ³é‡æ–°å›ç­”æˆ‘ä¸€æ¬¡", "files": []},
    {"text": "å¸®æˆ‘æœç´¢ä¸€ä¸‹å…»ç”ŸçŸ¥è¯†", "files": []},
        {"text": "å¸®æˆ‘ç”Ÿæˆä¸€å¼ è€äººç»ƒå¤ªæå›¾ç‰‡", "files": []},
    {
        "text": "å¸®æˆ‘ç”Ÿæˆä¸€ä»½ç”¨äºç§‘æ™®ç³–å°¿ç—…å‘ç—…åŸå› ï¼Œç—‡çŠ¶ï¼Œæ²»ç–—è¯ç‰©ï¼Œé¢„é˜²æªæ–½çš„PPT",
        "files": [],
    },
    {"text": "è¯·æ ¹æ®æˆ‘ç»™çš„å‚è€ƒèµ„æ–™ï¼Œç»™æˆ‘ä¸€ä¸ªåˆç†çš„é¥®é£Ÿå»ºè®®", "files": []},
    {"text": "è¯·æ ¹æ®æˆ‘ç»™çš„å‚è€ƒèµ„æ–™ï¼Œç”Ÿæˆä¸€ä¸ªç”¨äºç§‘æ™®åˆç†è†³é£Ÿçš„word", "files": []},
    {"text": "æˆ‘æœ€è¿‘æƒ³æ‰“å¤ªæå…»ç”Ÿï¼Œå¸®æˆ‘ç”Ÿæˆä¸€æ®µè€äººæ‰“å¤ªæçš„è§†é¢‘å§", "files": []},
    {"text": "æ ¹æ®æˆ‘çš„ç—…å†ï¼Œç»™æˆ‘ä¸€ä¸ªåˆç†çš„æ²»ç–—æ–¹æ¡ˆ", "files": []},
    {"text": "æ ¹æ®çŸ¥è¯†åº“ä»‹ç»ä¸€ä¸‹å¸¸è§ç–¾ç—…", "files": []},
    {"text": "æ ¹æ®çŸ¥è¯†å›¾è°±å‘Šè¯‰æˆ‘ç³–å°¿ç—…äººé€‚åˆåƒçš„é£Ÿç‰©æœ‰å“ªäº›ï¼Ÿ", "files": []},
]


# æ„å»º Gradio ç•Œé¢
with gr.Blocks() as demo:
    # æ ‡é¢˜å’Œæè¿°
    gr.Markdown("# ã€Œèµ›åšåä½—ã€ğŸ©º")

    # åˆ›å»ºèŠå¤©å¸ƒå±€
    with gr.Row():
        with gr.Column(scale=10):
            chatbot = gr.Chatbot(
                height=600,
                avatar_images=AVATAR,
                show_copy_button=True,
                latex_delimiters=[
                    {"left": "\\(", "right": "\\)", "display": True},
                    {"left": "\\[", "right": "\\]", "display": True},
                    {"left": "$$", "right": "$$", "display": True},
                    {"left": "$", "right": "$", "display": True},
                ],
                placeholder="\n## æ¬¢è¿ä¸æˆ‘å¯¹è¯ \nâ€”â€”â€”â€”æœ¬é¡¹ç›®å¼€æºåœ°å€https://github.com/Warma10032/cyber-doctor",
            )

    with gr.Row():
        with gr.Column(scale=9):
            chat_input = gr.MultimodalTextbox(
                interactive=True,
                file_count="multiple",
                placeholder="è¾“å…¥æ¶ˆæ¯æˆ–ä¸Šä¼ æ–‡ä»¶...",
                show_label=False,
            )
            audio_input = gr.Audio(
                sources=["microphone", "upload"],
                label="å½•éŸ³è¾“å…¥",
                visible=False,
                type="filepath",
            )
        with gr.Column(scale=1):
            clear = gr.ClearButton([chatbot, chat_input, audio_input], value="æ¸…é™¤è®°å½•")
            toggle_voice_button = gr.Button("è¯­éŸ³å¯¹è¯æ¨¡å¼", visible=True)
            toggle_text_button = gr.Button("æ–‡æœ¬äº¤æµæ¨¡å¼", visible=False)
            submit_audio_button = gr.Button("å‘é€", visible=False)

    with gr.Row() as example_row:
        example_component = gr.Examples(
            examples=examples, inputs=chat_input, visible=True, examples_per_page=15
        )

    chat_input.submit(fn=grodio_view, inputs=[chatbot, chat_input], outputs=[chatbot])
    # åˆ‡æ¢æŒ‰é’®ç‚¹å‡»äº‹ä»¶
    toggle_voice_button.click(
        fn=toggle_voice_mode,
        inputs=None,
        outputs=[
            chat_input,
            audio_input,
            toggle_voice_button,
            toggle_text_button,
            submit_audio_button,
        ],
    )

    toggle_text_button.click(
        fn=toggle_text_mode,
        inputs=None,
        outputs=[
            chat_input,
            audio_input,
            toggle_voice_button,
            toggle_text_button,
            submit_audio_button,
        ],
    )

    submit_audio_button.click(
        fn=gradio_audio_view, inputs=[chatbot, audio_input], outputs=[chatbot]
    )


# å¯åŠ¨åº”ç”¨
def start_gradio():
    demo.launch(server_port=10032, share=False)


if __name__ == "__main__":
    start_gradio()
